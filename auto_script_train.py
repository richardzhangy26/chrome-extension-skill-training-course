import requests
import json
import time
import os
import difflib
import math
import re
from datetime import datetime
from pathlib import Path
from openai import OpenAI
from typing import Optional, List, Dict
from workflow_tester_base import WorkflowTesterBase


class DialogueEntry:
    """å¯¹è¯æ—¥å¿—æ¡ç›®"""
    def __init__(self, timestamp: str, step_id: str, source: str,
                 ai_text: Optional[str] = None, user_text: Optional[str] = None,
                 round_num: Optional[int] = None):
        self.timestamp = timestamp
        self.step_id = step_id
        self.source = source  # "runCard" æˆ– "chat"
        self.ai_text = ai_text
        self.user_text = user_text
        self.round_num = round_num

    def __repr__(self):
        return f"DialogueEntry(timestamp={self.timestamp}, step_id={self.step_id}, " \
               f"source={self.source}, round={self.round_num})"


class DialogueLogParser:
    """å¯¹è¯æ—¥å¿—è§£æå™¨"""

    @staticmethod
    def parse_log_file(log_path: str) -> List[DialogueEntry]:
        """
        è§£æå¯¹è¯æ—¥å¿—æ–‡ä»¶

        Args:
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„

        Returns:
            è§£æåçš„å¯¹è¯æ¡ç›®åˆ—è¡¨
        """
        entries = []

        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
            return entries

        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²å¯¹è¯å—ï¼ˆå¤„ç†å¯èƒ½çš„æ¢è¡Œç¬¦å·®å¼‚ï¼‰
        separator = '-' * 80
        # æ›¿æ¢æ‰€æœ‰å¯èƒ½çš„åˆ†éš”ç¬¦å˜ä½“ä¸ºç»Ÿä¸€æ ¼å¼
        normalized_content = content.replace(separator + '\r\n', separator + '\n')
        normalized_content = normalized_content.replace(separator + '\r', separator + '\n')
        blocks = normalized_content.split(separator + '\n')

        for block in blocks:
            if not block.strip():
                continue

            entry = DialogueLogParser._parse_block(block)
            if entry:
                entries.append(entry)

        print(f"âœ… è§£ææ—¥å¿—æ–‡ä»¶å®Œæˆï¼Œå…± {len(entries)} ä¸ªå¯¹è¯æ¡ç›®")
        return entries

    @staticmethod
    def _parse_block(block: str) -> Optional[DialogueEntry]:
        """è§£æå•ä¸ªå¯¹è¯å—"""
        lines = block.strip().split('\n')
        if not lines:
            return None

        # è§£æå¤´éƒ¨ä¿¡æ¯
        header = lines[0]
        timestamp, step_id, round_num, source = DialogueLogParser._parse_header(header)

        # è§£æç”¨æˆ·å’ŒAIæ–‡æœ¬
        ai_text = None
        user_text = None

        for line in lines[1:]:
            line = line.strip()
            if line.startswith('AI:'):
                ai_text = line[3:].strip()
            elif line.startswith('ç”¨æˆ·:'):
                user_text = line[3:].strip()

        return DialogueEntry(
            timestamp=timestamp,
            step_id=step_id,
            source=source,
            ai_text=ai_text,
            user_text=user_text,
            round_num=round_num
        )

    @staticmethod
    def _parse_header(header: str) -> tuple:
        """è§£æå¤´éƒ¨ä¿¡æ¯"""
        # æ–°æ ¼å¼: [2025-11-28 16:01:21] Step: æ­¥éª¤åç§° | step_id: GnxX4RzREzTrXNmRGxq0 | ç¬¬ 1 è½® | æ¥æº: chat
        # æ—§æ ¼å¼: [2025-11-28 16:01:21] Step GnxX4RzREzTrXNmRGxq0 | ç¬¬ 1 è½® | æ¥æº: chat
        timestamp = ""
        step_id = ""
        round_num = None
        source = "chat"

        try:
            # æå–æ—¶é—´æˆ³
            if header.startswith('['):
                end_idx = header.find(']')
                if end_idx > 0:
                    timestamp = header[1:end_idx].strip()

            # ä¼˜å…ˆå°è¯•æ–°æ ¼å¼ï¼šæå– step_id: xxx
            step_id_marker = 'step_id: '
            step_id_start = header.find(step_id_marker)
            if step_id_start > 0:
                # æ–°æ ¼å¼
                step_id_value_start = step_id_start + len(step_id_marker)
                step_id_end = header.find(' |', step_id_value_start)
                if step_id_end > 0:
                    step_id = header[step_id_value_start:step_id_end].strip()
                else:
                    # step_id å¯èƒ½åœ¨æœ«å°¾
                    step_id = header[step_id_value_start:].strip()
            else:
                # å…¼å®¹æ—§æ ¼å¼ï¼šStep xxx |
                step_start = header.find('Step ')
                if step_start > 0:
                    step_end = header.find(' |', step_start)
                    if step_end > 0:
                        step_id = header[step_start + 5:step_end].strip()

            # æå–è½®æ¬¡
            round_start = header.find('ç¬¬ ')
            if round_start > 0:
                round_end = header.find(' è½®', round_start)
                if round_end > 0:
                    round_str = header[round_start + 2:round_end].strip()
                    try:
                        round_num = int(round_str)
                    except ValueError:
                        round_num = None

            # æå–æ¥æº
            source_start = header.find('æ¥æº: ')
            if source_start > 0:
                source = header[source_start + 4:].strip()
        except Exception as e:
            print(f"âš ï¸  è§£æå¤´éƒ¨ä¿¡æ¯å¤±è´¥: {header}, é”™è¯¯: {str(e)}")

        return timestamp, step_id, round_num, source

    @staticmethod
    def extract_dialogue_pairs(entries: List[DialogueEntry]) -> List[Dict]:
        """
        ä»å¯¹è¯æ¡ç›®ä¸­æå–AIæé—®-ç”¨æˆ·å›ç­”å¯¹

        Args:
            entries: å¯¹è¯æ¡ç›®åˆ—è¡¨

        Returns:
            [{"ai": ai_text, "user": user_text}, ...]
        """
        # Important: chat blocks contain A_i (user) and Q_{i+1} (AI).
        # We pair each user answer with the most recent AI question seen earlier.
        pairs: List[Dict] = []
        last_ai_text: Optional[str] = None
        last_ai_meta: Dict = {}

        for entry in entries:
            if entry.user_text and last_ai_text:
                pairs.append({
                    "ai": last_ai_text,
                    "user": entry.user_text,
                    "timestamp": entry.timestamp,
                    "step_id": last_ai_meta.get("step_id") or entry.step_id,
                    "round_num": entry.round_num,
                })

            if entry.ai_text:
                last_ai_text = entry.ai_text
                last_ai_meta = {
                    "timestamp": entry.timestamp,
                    "step_id": entry.step_id,
                    "round_num": entry.round_num,
                }

        print(f"âœ… æå–åˆ° {len(pairs)} ä¸ªå¯¹è¯å¯¹")
        return pairs


class DialogueMatcher:
    """å¯¹è¯åŒ¹é…å™¨"""

    def __init__(self, similarity_threshold: float = 0.7):
        """
        åˆå§‹åŒ–åŒ¹é…å™¨

        Args:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.7
        """
        self.threshold = similarity_threshold

    def find_best_match(
        self,
        ai_question: str,
        dialogue_pairs: List[Dict],
        step_id: Optional[str] = None,
    ) -> Optional[str]:
        """
        æŸ¥æ‰¾æœ€ä½³åŒ¹é…çš„ç”¨æˆ·å›ç­”

        Args:
            ai_question: å½“å‰AIæé—®
            dialogue_pairs: å†å²å¯¹è¯å¯¹åˆ—è¡¨

        Returns:
            åŒ¹é…çš„ç”¨æˆ·å›ç­”ï¼Œæˆ–Noneè¡¨ç¤ºæœªæ‰¾åˆ°
        """
        if not dialogue_pairs:
            return None

        candidates = dialogue_pairs
        if step_id:
            step_candidates = [p for p in dialogue_pairs if p.get("step_id") == step_id]
            if step_candidates:
                candidates = step_candidates

        best_match = None
        best_similarity = 0.0
        best_pair_info = None

        for pair in candidates:
            historical_ai = pair.get("ai", "")
            if not historical_ai:
                continue

            similarity = self.calculate_similarity(ai_question, historical_ai)

            if similarity > best_similarity and similarity >= self.threshold:
                best_similarity = similarity
                best_match = pair.get("user")
                best_pair_info = {
                    "similarity": similarity,
                    "historical_ai": historical_ai,
                    "timestamp": pair.get("timestamp"),
                    "step_id": pair.get("step_id"),
                    "round_num": pair.get("round_num")
                }

        if best_match:
            print(f"âœ… æ‰¾åˆ°åŒ¹é…å›ç­”ï¼Œç›¸ä¼¼åº¦: {best_similarity:.2f}")
            if best_pair_info:
                print(f"   åŸå§‹AIæé—®: {best_pair_info['historical_ai'][:50]}...")
                print(f"   æ—¶é—´: {best_pair_info.get('timestamp')}, æ­¥éª¤: {best_pair_info.get('step_id')}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…å›ç­” (æœ€é«˜ç›¸ä¼¼åº¦: {best_similarity:.2f}, é˜ˆå€¼: {self.threshold})")

        return best_match

    @staticmethod
    def calculate_similarity(text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        if not text1 or not text2:
            return 0.0

        # é¢„å¤„ç†ï¼šå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œç¬¦
        text1_clean = ' '.join(text1.split())
        text2_clean = ' '.join(text2.split())

        # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
        return difflib.SequenceMatcher(None, text1_clean, text2_clean).ratio()


class DialogueReplayEngine:
    """å¯¹è¯å›æ”¾å¼•æ“"""

    def __init__(self, log_path: str, similarity_threshold: float = 0.7):
        """
        åˆå§‹åŒ–å›æ”¾å¼•æ“

        Args:
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.log_path = log_path
        self.threshold = similarity_threshold
        self.parser = DialogueLogParser()
        self.matcher = DialogueMatcher(similarity_threshold)
        self.dialogue_pairs = None
        self.loaded = False

    def load_log(self) -> bool:
        """åŠ è½½å’Œè§£ææ—¥å¿—æ–‡ä»¶"""
        try:
            entries = self.parser.parse_log_file(self.log_path)
            self.dialogue_pairs = self.parser.extract_dialogue_pairs(entries)
            self.loaded = True
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ—¥å¿—å¤±è´¥: {str(e)}")
            return False

    def get_answer(self, ai_question: str, step_id: Optional[str] = None) -> Optional[str]:
        """
        è·å–åŒ¹é…çš„å›ç­”

        Args:
            ai_question: AIæé—®
            step_id: å½“å‰æ­¥éª¤IDï¼ˆå¯é€‰ï¼Œç”¨äºè¿‡æ»¤å€™é€‰ï¼‰

        Returns:
            åŒ¹é…çš„ç”¨æˆ·å›ç­”ï¼Œæˆ–Noneè¡¨ç¤ºæœªæ‰¾åˆ°
        """
        if not self.loaded or not self.dialogue_pairs:
            print("âš ï¸  æ—¥å¿—æœªåŠ è½½æˆ–ä¸ºç©º")
            return None

        return self.matcher.find_best_match(ai_question, self.dialogue_pairs, step_id=step_id)

    def get_match_info(self, ai_question: str, step_id: Optional[str] = None) -> Dict:
        """
        è·å–åŒ¹é…çš„è¯¦ç»†ä¿¡æ¯

        Args:
            ai_question: AIæé—®
            step_id: å½“å‰æ­¥éª¤IDï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ¹é…ä¿¡æ¯å­—å…¸
        """
        if not self.loaded or not self.dialogue_pairs:
            return {"error": "æ—¥å¿—æœªåŠ è½½æˆ–ä¸ºç©º"}

        best_match = None
        best_similarity = 0.0
        best_pair = None

        candidates = self.dialogue_pairs
        if step_id:
            step_candidates = [p for p in self.dialogue_pairs if p.get("step_id") == step_id]
            if step_candidates:
                candidates = step_candidates

        for pair in candidates:
            historical_ai = pair.get("ai", "")
            if not historical_ai:
                continue

            similarity = self.matcher.calculate_similarity(ai_question, historical_ai)

            if similarity > best_similarity:
                best_similarity = similarity
                best_match = pair.get("user")
                best_pair = pair

        return {
            "matched": best_similarity >= self.threshold,
            "similarity": best_similarity,
            "answer": best_match,
            "threshold": self.threshold,
            "historical_ai": best_pair.get("ai") if best_pair else None,
            "timestamp": best_pair.get("timestamp") if best_pair else None,
            "step_id": best_pair.get("step_id") if best_pair else None,
            "round_num": best_pair.get("round_num") if best_pair else None,
            "total_pairs": len(candidates)
        }


class EmbeddingClient:
    """OpenAI-compatible embedding client using api-key header."""

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://llm-service.polymas.com/api/openai/v1",
        model: str = "text-embedding-3-small",
        max_batch_size: int = 25,
        timeout: int = 60,
    ):
        self.api_key = api_key
        base_url = base_url.rstrip("/")
        self.embed_url = base_url if base_url.endswith("/embeddings") else base_url + "/embeddings"
        self.model = model
        self.max_batch_size = max_batch_size
        self.timeout = timeout
        self.session = requests.Session()

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []

        embeddings: List[List[float]] = []
        headers = {
            "Content-Type": "application/json",
            "api-key": self.api_key,
        }

        for i in range(0, len(texts), self.max_batch_size):
            batch = texts[i : i + self.max_batch_size]
            payload = {"input": batch, "model": self.model}
            resp = self.session.post(self.embed_url, json=payload, headers=headers, timeout=self.timeout)
            resp.raise_for_status()
            data = resp.json() or {}
            items = data.get("data") or []
            # Ensure original ordering by index if provided.
            items = sorted(items, key=lambda x: x.get("index", 0))
            embeddings.extend([it.get("embedding") for it in items])

        return embeddings


class JsonDialogueReplayEngine:
    """Replay engine based on exported dialogue JSON + embeddings."""

    def __init__(
        self,
        json_path: str,
        similarity_threshold: float = 0.8,
        embedding_model: str = "text-embedding-3-large",
        embedding_base_url: str = "https://llm-service.polymas.com/api/openai/v1",
    ):
        self.json_path = json_path
        self.threshold = similarity_threshold
        self.embedding_model = embedding_model
        self.embedding_base_url = embedding_base_url
        self.dialogue_pairs: List[Dict] = []
        self.loaded = False
        self.embed_client: Optional[EmbeddingClient] = None
        self._last_query_key = None
        self._last_match_info: Optional[Dict] = None

    @staticmethod
    def _normalize_question(text: str) -> str:
        # Strip think tags / artifacts.
        text = re.sub(r"</?think[^>]*>", "", text or "")
        text = text.strip()
        if not text:
            return text
        # Take the last sentence ending with '?' or 'ï¼Ÿ' to reduce noise.
        matches = re.findall(r"[^ã€‚ï¼ï¼Ÿ\n\r]*[ï¼Ÿ\?]", text)
        if matches:
            return matches[-1].strip()
        return text

    def _parse_json_pairs(self, data: Dict) -> List[Dict]:
        pairs: List[Dict] = []
        last_ai_raw: Optional[str] = None
        last_ai_norm: Optional[str] = None
        last_step_id: Optional[str] = None
        last_stage_index: Optional[int] = None

        for stage in data.get("stages", []) or []:
            step_id = stage.get("step_id") or stage.get("stepId")
            stage_index = stage.get("stage_index") or stage.get("stageIndex")
            for m in stage.get("messages", []) or []:
                role = m.get("role")
                content = (m.get("content") or "").strip()
                if not content:
                    continue
                if role == "assistant":
                    last_ai_raw = content
                    last_ai_norm = self._normalize_question(content)
                    last_step_id = step_id
                    last_stage_index = stage_index
                elif role == "user" and last_ai_norm:
                    pairs.append({
                        "ai": last_ai_norm,
                        "ai_raw": last_ai_raw,
                        "user": content,
                        "step_id": last_step_id,
                        "round_num": m.get("round"),
                        "stage_index": last_stage_index,
                    })
        return pairs

    def load_log(self) -> bool:
        try:
            with open(self.json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å– JSON å›æ”¾æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

        self.dialogue_pairs = self._parse_json_pairs(data)
        if not self.dialogue_pairs:
            print("âš ï¸  JSON ä¸­æœªæå–åˆ°å¯ç”¨å¯¹è¯å¯¹")
            return False

        # Try load cached embeddings to avoid recomputation.
        cache_path = Path(self.json_path).with_name(Path(self.json_path).stem + "_replay_index.json")
        if cache_path.exists():
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    cached = json.load(f)
                if isinstance(cached, list) and all("emb" in p for p in cached):
                    self.dialogue_pairs = cached
                    self.loaded = True
                    print(f"âœ… å·²åŠ è½½ embedding ç´¢å¼•ç¼“å­˜: {str(cache_path)}")
                    return True
            except Exception:
                pass

        api_key = os.getenv("EMBEDDING_API_KEY") or os.getenv("ARK_API_KEY")
        if not api_key:
            print("âŒ æœªè®¾ç½® EMBEDDING_API_KEYï¼Œæ— æ³•ç”Ÿæˆ embedding")
            return False

        self.embed_client = EmbeddingClient(
            api_key=api_key,
            base_url=self.embedding_base_url,
            model=self.embedding_model,
            max_batch_size=6 if "embedding-v3" in self.embedding_model or self.embedding_model.endswith("v3") else 25,
        )

        try:
            texts = [p["ai"] for p in self.dialogue_pairs]
            embs = self.embed_client.embed_texts(texts)
            if len(embs) != len(self.dialogue_pairs):
                print("âš ï¸  embedding æ•°é‡ä¸å¯¹è¯å¯¹æ•°é‡ä¸ä¸€è‡´ï¼Œå°†å›é€€åˆ°æ™®é€šæ¨¡å¼")
                return False
            for p, e in zip(self.dialogue_pairs, embs):
                p["emb"] = e
            # Write cache.
            try:
                with open(cache_path, "w", encoding="utf-8") as f:
                    json.dump(self.dialogue_pairs, f, ensure_ascii=False)
                print(f"âœ… å·²å†™å…¥ embedding ç´¢å¼•ç¼“å­˜: {str(cache_path)}")
            except Exception:
                pass
            self.loaded = True
            return True
        except Exception as e:
            print(f"âŒ ç”Ÿæˆ embedding å¤±è´¥: {str(e)}")
            return False

    @staticmethod
    def _cosine(a: List[float], b: List[float]) -> float:
        dot = sum(x * y for x, y in zip(a, b))
        na = math.sqrt(sum(x * x for x in a))
        nb = math.sqrt(sum(y * y for y in b))
        return dot / (na * nb + 1e-9)

    def get_answer(self, ai_question: str, step_id: Optional[str] = None) -> Optional[str]:
        if not self.loaded or not self.dialogue_pairs or not self.embed_client:
            print("âš ï¸  JSON å›æ”¾å¼•æ“æœªåŠ è½½")
            return None

        q_norm = self._normalize_question(ai_question)
        q_emb = self.embed_client.embed_texts([q_norm])[0]

        candidates = self.dialogue_pairs
        if step_id:
            step_candidates = [p for p in self.dialogue_pairs if p.get("step_id") == step_id]
            if step_candidates:
                candidates = step_candidates

        best_pair = None
        best_sim = 0.0
        for p in candidates:
            emb = p.get("emb")
            if not emb:
                continue
            sim = self._cosine(q_emb, emb)
            if sim > best_sim:
                best_sim = sim
                best_pair = p

        self._last_query_key = (ai_question, step_id)
        self._last_match_info = {
            "matched": bool(best_pair and best_sim >= self.threshold),
            "similarity": best_sim,
            "answer": best_pair.get("user") if best_pair else None,
            "threshold": self.threshold,
            "historical_ai": (best_pair.get("ai_raw") or best_pair.get("ai")) if best_pair else None,
            "step_id": best_pair.get("step_id") if best_pair else None,
            "round_num": best_pair.get("round_num") if best_pair else None,
            "total_pairs": len(candidates),
        }

        if best_pair and best_sim >= self.threshold:
            print(f"âœ… JSON å›æ”¾å‘½ä¸­ï¼Œç›¸ä¼¼åº¦: {best_sim:.3f}")
            return best_pair.get("user")

        print(f"âŒ JSON å›æ”¾æœªå‘½ä¸­ (æœ€é«˜ç›¸ä¼¼åº¦: {best_sim:.3f}, é˜ˆå€¼: {self.threshold})")
        return None

    def get_match_info(self, ai_question: str, step_id: Optional[str] = None) -> Dict:
        key = (ai_question, step_id)
        if self._last_query_key == key and self._last_match_info:
            return self._last_match_info
        # Fallback: run a match to populate info.
        _ = self.get_answer(ai_question, step_id=step_id)
        return self._last_match_info or {"matched": False, "similarity": 0.0}


class WorkflowTester(WorkflowTesterBase):
    DEFAULT_PROFILE_KEY = "medium"
    PROFILE_LABEL_FIELD_NAME = "å­¦ç”Ÿæ¡£ä½"
    PROFILE_SELECT_TITLE = "å­¦ç”Ÿæ¡£ä½"

    STUDENT_PROFILES = {
        "good": {
            "label": "ä¼˜ç§€å­¦ç”Ÿ",
            "description": "ç†è§£é€å½»ã€è¡¨è¾¾æ¸…æ™°ï¼Œå›ç­”ç»“æ„åŒ–ã€æ¡ç†åˆ†æ˜ï¼Œå¹¶ä¸»åŠ¨æ€»ç»“è¦ç‚¹ã€‚",
            "style": "è¯­æ°”è‡ªä¿¡ã€è¯­è¨€è§„èŒƒï¼Œå¿…è¦æ—¶å¼•ç”¨é¢˜ç›®æˆ–ææ–™ä¸­çš„å…³é”®ä¿¡æ¯ã€‚",
            "fallback_hint": "è‹¥æ¨¡æ‹Ÿå¯¹è¯ä¸­æ²¡æœ‰åˆé€‚ç¤ºä¾‹ï¼Œå¯è‡ªå·±ç»„ç»‡æœ€ä½³ç­”æ¡ˆï¼Œä¿æŒé«˜æ°´å¹³ã€‚"
        },
        "medium": {
            "label": "éœ€è¦å¼•å¯¼çš„å­¦ç”Ÿ",
            "description": "åŸºæœ¬ç†è§£é—®é¢˜ä½†ä¸å¤Ÿå…¨é¢ï¼Œå›ç­”ä¸­ä¼šæš´éœ²ç–‘æƒ‘æˆ–è¯·æ±‚æç¤ºã€‚",
            "style": "è¯­æ°”ç•¥æ˜¾çŠ¹è±«ï¼Œèƒ½è¦†ç›–æ ¸å¿ƒå†…å®¹ï¼Œä½†ä¼šæå‡º 1-2 ä¸ªä¸ç¡®å®šç‚¹æˆ–å¯»æ±‚è€å¸ˆå»ºè®®ã€‚",
            "fallback_hint": "ç¤ºä¾‹ç¼ºå¤±æ—¶ï¼Œå…ˆå›ç­”ä¸»è¦å†…å®¹å†è¯´æ˜ä¸ç¡®å®šä¹‹å¤„ã€‚"
        },
        "bad": {
            "label": "ç­”éæ‰€é—®çš„å­¦ç”Ÿ",
            "description": "ç†è§£åå·®ï¼Œå¸¸å¸¸è·‘é¢˜æˆ–åªå¤è¿°ä¸é—®é¢˜å¼±ç›¸å…³çš„ä¿¡æ¯ã€‚",
            "style": "è¯­æ°”éšæ„ï¼Œå®¹æ˜“åç¦»é‡ç‚¹æˆ–ç­”éæ‰€é—®ã€‚",
            "fallback_hint": "å³ä½¿éœ€è¦è‡ªå·±ç”Ÿæˆï¼Œä¹Ÿè¦ä¿æŒè½»å¾®è·‘é¢˜æˆ–è¯¯è§£çš„ç‰¹å¾ã€‚"
        }
    }

    def __init__(self, base_url="https://cloudapi.polymas.com"):
        super().__init__(base_url)

        # Provide profile data for base prompt/selection helpers.
        self.student_profiles = self.STUDENT_PROFILES

        # é‡è¯•é…ç½®
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.base_timeout = 60  # åŸºç¡€è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.retry_backoff = 2  # é‡è¯•é€€é¿å› å­

        # æ¨¡å‹é…ç½®
        self.model_type = os.getenv("MODEL_TYPE", "doubao_sdk")  # doubao_sdk, doubao_post
        self.doubao_client = None
        self.doubao_model = os.getenv("DOUBAO_MODEL", "doubao-seed-1-6-251015")

        # POST è°ƒç”¨é…ç½®
        self.llm_api_url = os.getenv(
            "LLM_API_URL",
            "http://llm-service.polymas.com/api/openai/v1/chat/completions",
        )
        self.llm_api_key = os.getenv("LLM_API_KEY", "")
        self.llm_model = os.getenv("LLM_MODEL", "Doubao-1.5-pro-32k")
        self.llm_service_code = os.getenv("LLM_SERVICE_CODE", "SI_Ability")

        # å›æ”¾æ¨¡å¼ç›¸å…³å±æ€§
        self.replay_engine = None
        self.use_replay_mode = False
        self.similarity_threshold = 0.7
        self.replay_log_path = None

        self._initialize_doubao_client()

    def _initialize_doubao_client(self):
        """åˆå§‹åŒ– Doubao å®¢æˆ·ç«¯"""
        print(f"ğŸ”§ æ¨¡å‹ç±»å‹: {self.model_type}")

        if self.model_type == "doubao_post":
            print(f"   - ä½¿ç”¨ Doubao POST API è°ƒç”¨æ¨¡å¼")
            print(f"   - API URL: {self.llm_api_url}")
            print(f"   - Model: {self.llm_model}")
            print(f"   - Service Code: {self.llm_service_code}")
            if not self.llm_api_key:
                print("âš ï¸  è­¦å‘Š: LLM_API_KEY æœªè®¾ç½®")

        elif self.model_type == "doubao_sdk":
            api_key = os.getenv("ARK_API_KEY")
            base_url = os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")

            if api_key:
                try:
                    self.doubao_client = OpenAI(api_key=api_key, base_url=base_url)
                    print(f"   - ä½¿ç”¨ Doubao OpenAI SDK è°ƒç”¨æ¨¡å¼")
                    print(f"   - Model: {self.doubao_model}")
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: åˆå§‹åŒ– Doubao å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            else:
                print("âš ï¸  è­¦å‘Š: ARK_API_KEY æœªè®¾ç½®")
        else:
            print(f"âš ï¸  è­¦å‘Š: æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}")

    def _call_doubao_post(self, messages, temperature=0.7, max_tokens=1000):
        """ä½¿ç”¨ HTTP POST æ–¹å¼è°ƒç”¨ Doubao API"""
        headers = {
            "Content-Type": "application/json",
            "service-code": self.llm_service_code,
        }

        if self.llm_api_key:
            headers["api-key"] = self.llm_api_key

        payload = {
            "model": self.llm_model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": 0.9,
            "frequency_penalty": 0.3,
            "presence_penalty": 0.2
        }

        try:
            response = requests.post(
                self.llm_api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
        except requests.exceptions.RequestException as e:
            print(f"âŒ HTTP POST è°ƒç”¨å¤±è´¥: {str(e)}")
            return None
        except (KeyError, IndexError) as e:
            print(f"âŒ è§£æå“åº”å¤±è´¥: {str(e)}")
            return None

    def _retry_request(self, request_func, *args, **kwargs):
        """
        é€šç”¨é‡è¯•æœºåˆ¶

        Args:
            request_func: è¦æ‰§è¡Œçš„è¯·æ±‚å‡½æ•°
            *args, **kwargs: ä¼ é€’ç»™è¯·æ±‚å‡½æ•°çš„å‚æ•°

        Returns:
            è¯·æ±‚ç»“æœ
        """
        last_exception = None

        for attempt in range(self.max_retries):
            try:
                # åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´
                timeout = self.base_timeout * (attempt + 1)
                if 'timeout' in kwargs:
                    kwargs['timeout'] = timeout

                print(f"ğŸ”„ å°è¯•ç¬¬ {attempt + 1}/{self.max_retries} æ¬¡è¯·æ±‚ (è¶…æ—¶: {timeout}ç§’)...")

                result = request_func(*args, **kwargs)

                # å¦‚æœæˆåŠŸï¼Œè¿”å›ç»“æœ
                if attempt > 0:
                    print(f"âœ… é‡è¯•æˆåŠŸï¼")
                return result

            except requests.exceptions.ReadTimeout as e:
                last_exception = e
                print(f"âš ï¸  è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")

                if attempt < self.max_retries - 1:
                    # è®¡ç®—é€€é¿ç­‰å¾…æ—¶é—´
                    wait_time = self.retry_backoff ** attempt
                    print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

            except requests.exceptions.RequestException as e:
                # å…¶ä»–ç½‘ç»œé”™è¯¯ï¼Œä¸é‡è¯•
                print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
                raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"è¯·æ±‚è¶…æ—¶ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡")

    def _post_json(self, url: str, payload: Dict, timeout: int):
        """Override base POST to add retries."""
        def make_request():
            return self.session.post(
                url,
                json=payload,
                headers=self.headers,
                timeout=timeout,
            )
        return self._retry_request(make_request)

    def _log_run_card(self, step_id, payload, response_data):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        step_name = self._get_step_display_name(step_id)
        # åŒæ—¶è®°å½• step_name å’Œ step_idï¼Œä¾¿äºé˜…è¯»å’Œå›æ”¾
        log_lines = [
            f"[{timestamp}] Step: {step_name} | step_id: {step_id}",
            f"è¯·æ±‚è½½è·: {json.dumps(payload, ensure_ascii=False)}",
            f"å“åº”å†…å®¹: {json.dumps(response_data, ensure_ascii=False)}",
            "-" * 80,
        ]
        self._append_log(self.run_card_log_path, "\n".join(log_lines))

    def _log_dialogue_entry(self, step_id, user_text=None, ai_text=None, source="chat"):
        if user_text is None and ai_text is None:
            return
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        step_name = self._get_step_display_name(step_id)
        round_info = f" | ç¬¬ {self.dialogue_round} è½®" if self.dialogue_round else ""
        # åŒæ—¶è®°å½• step_name å’Œ step_idï¼Œä¾¿äºé˜…è¯»å’Œå›æ”¾
        header = f"[{timestamp}] Step: {step_name} | step_id: {step_id}{round_info} | æ¥æº: {source}"
        lines = [header]
        if user_text:
            lines.append(f"ç”¨æˆ·: {user_text}")
        if ai_text:
            lines.append(f"AI: {ai_text}")
        lines.append("-" * 80)
        self._append_log(self.dialogue_log_path, "\n".join(lines))

        # Collect JSON stage data when enabled (base hook).
        if user_text:
            try:
                self._collect_stage_data(step_id, self.dialogue_round, "user", user_text)
            except Exception:
                pass
        if ai_text:
            try:
                self._collect_stage_data(step_id, self.dialogue_round, "assistant", ai_text)
            except Exception:
                pass

    def enable_replay_mode(self, log_path: str, similarity_threshold: float = 0.7):
        """
        å¯ç”¨å›æ”¾æ¨¡å¼

        Args:
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.7
        """
        self.use_replay_mode = True
        self.replay_log_path = log_path
        self.similarity_threshold = similarity_threshold

        # åˆ›å»ºå›æ”¾å¼•æ“ï¼šæ”¯æŒ txt(difflib) ä¸ json(embedding) ä¸¤ç§æ ¼å¼
        if str(log_path).lower().endswith(".json"):
            emb_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            emb_base_url = os.getenv(
                "EMBEDDING_BASE_URL",
                "https://llm-service.polymas.com/api/openai/v1",
            )
            self.replay_engine = JsonDialogueReplayEngine(
                log_path,
                similarity_threshold=similarity_threshold,
                embedding_model=emb_model,
                embedding_base_url=emb_base_url,
            )
        else:
            self.replay_engine = DialogueReplayEngine(log_path, similarity_threshold)

        # åŠ è½½æ—¥å¿—
        if self.replay_engine.load_log():
            print(f"\nğŸ¯ å·²å¯ç”¨å›æ”¾æ¨¡å¼")
            print(f"   æ—¥å¿—æ–‡ä»¶: {log_path}")
            print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
            print(f"   åŠ è½½å¯¹è¯å¯¹: {len(self.replay_engine.dialogue_pairs or [])} ä¸ª")
        else:
            print(f"\nâŒ å›æ”¾æ¨¡å¼å¯ç”¨å¤±è´¥ï¼Œå°†ä½¿ç”¨æ™®é€šæ¨¡å¼")
            self.use_replay_mode = False
            self.replay_engine = None

    def generate_answer_with_replay(self, question: str) -> str:
        """
        ä¼˜å…ˆä½¿ç”¨æ—¥å¿—å›ç­”ï¼Œå›é€€åˆ°æ¨¡å‹ç”Ÿæˆ

        Args:
            question: AIæé—®

        Returns:
            ç”¨æˆ·å›ç­”
        """
        if not self.use_replay_mode or not self.replay_engine:
            print("âš ï¸  æœªå¯ç”¨å›æ”¾æ¨¡å¼ï¼Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”")
            return self.generate_answer_with_doubao(question)

        # å°è¯•ä»æ—¥å¿—ä¸­è·å–åŒ¹é…çš„å›ç­”
        step_id = getattr(self, "current_step_id", None)
        matched_answer = self.replay_engine.get_answer(question, step_id=step_id)

        if matched_answer:
            print(f"ğŸ¯ ä½¿ç”¨æ—¥å¿—å›ç­” (ç›¸ä¼¼åº¦åŒ¹é…)")
            return matched_answer
        else:
            print("ğŸ” æœªæ‰¾åˆ°åŒ¹é…çš„æ—¥å¿—å›ç­”ï¼Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆ")
            return self.generate_answer_with_doubao(question)

    def generate_answer_with_doubao(self, question):
        """ä½¿ç”¨ Doubao æ¨¡å‹ç”Ÿæˆå›ç­”"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„è°ƒç”¨æ–¹å¼
        if self.model_type == "doubao_sdk" and not self.doubao_client:
            print("âŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        elif self.model_type == "doubao_post" and not self.llm_api_url:
            print("âŒ POST API URL æœªé…ç½®")
            return None

        try:
            profile_info = self._get_student_profile_info()
            system_prompt = (
                "ä½ æ˜¯ä¸€åèƒ½åŠ›è®­ç»ƒåŠ©æ‰‹ï¼Œéœ€è¦ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„å­¦ç”Ÿæ¡£ä½æ‰®æ¼”è§’è‰²ã€‚"
            )

            sections = [
                "## è§’è‰²è®¾å®š",
                f"å­¦ç”Ÿæ¡£ä½: {profile_info['label']}",
                f"è§’è‰²ç‰¹å¾: {profile_info['description']}",
                f"è¡¨è¾¾é£æ ¼: {profile_info['style']}",
                "",
            ]

            # æ·»åŠ é—®é¢˜ç±»å‹è¯†åˆ«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            sections.extend([
                "## é—®é¢˜ç±»å‹è¯†åˆ«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰",
                "å¦‚æœå½“å‰é—®é¢˜å±äºä»¥ä¸‹ç±»å‹ï¼Œè¯·ä¼˜å…ˆç›´æ¥å›ç­”ï¼Œä¸éœ€è¦å¼ºåˆ¶ä½“ç°æ€§æ ¼ç‰¹ç‚¹ï¼š",
                "1. **ç¡®è®¤å¼é—®é¢˜**: å¦‚'ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿè¯·å›å¤æ˜¯æˆ–å¦'ã€'ç¡®è®¤çš„è¯è¯·å›å¤æ˜¯'",
                "   â†’ ç›´æ¥å›ç­”'æ˜¯'ã€'å¥½çš„'ã€'ç¡®è®¤'ç­‰",
                "2. **é€‰æ‹©å¼é—®é¢˜**: å¦‚'ä½ é€‰æ‹©Aè¿˜æ˜¯Bï¼Ÿ'ã€'è¯·é€‰æ‹©1/2/3'",
                "   â†’ ç›´æ¥è¯´å‡ºé€‰é¡¹ï¼Œå¦‚'æˆ‘é€‰æ‹©A'ã€'é€‰1'",
                "3. **è§’è‰²ç¡®è®¤é—®é¢˜**: å¦‚'ä½ æ˜¯å­¦ç”Ÿè¿˜æ˜¯è€å¸ˆï¼Ÿ'",
                "   â†’ ç›´æ¥å›ç­”è§’è‰²ï¼Œå¦‚'å­¦ç”Ÿ'",
                "",
                "**åˆ¤æ–­æ ‡å‡†**: å¦‚æœé—®é¢˜ä¸­åŒ…å«'è¯·å›å¤'ã€'è¯·é€‰æ‹©'ã€'æ˜¯æˆ–å¦'ã€'A/B/C'ç­‰æ˜ç¡®æŒ‡ç¤ºï¼Œåˆ™ä¸ºå°é—­å¼é—®é¢˜ã€‚",
                "",
            ])

            if self.dialogue_samples_content:
                sections.extend([
                    "## æ¡£ä½ç¤ºä¾‹å¯¹è¯ (å¦‚æœ‰åŒ¹é…è¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™ï¼Œä¼˜å…ˆçº§æœ€é«˜)",
                    self.dialogue_samples_content,
                    "",
                ])

            if self.knowledge_base_content:
                sections.extend([
                    "## å‚è€ƒçŸ¥è¯†åº“ (å¯ç»“åˆä½¿ç”¨)",
                    self.knowledge_base_content,
                    "",
                ])

            # æ·»åŠ å¯¹è¯å†å²
            if self.conversation_history:
                sections.extend([
                    "## å¯¹è¯å†å²ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰",
                ])
                for i, turn in enumerate(self.conversation_history, 1):
                    sections.append(f"ç¬¬{i}è½®:")
                    sections.append(f"  AIæé—®: {turn['ai']}")
                    sections.append(f"  å­¦ç”Ÿå›ç­”: {turn['student']}")
                sections.append("")

            sections.extend([
                "## å½“å‰é—®é¢˜",
                question,
                "",
                "## è¾“å‡ºè¦æ±‚ï¼ˆæŒ‰ä¼˜å…ˆçº§æ‰§è¡Œï¼‰",
                "**ä¼˜å…ˆçº§1**: å¦‚æœæ˜¯å°é—­å¼é—®é¢˜ï¼ˆç¡®è®¤å¼/é€‰æ‹©å¼/è§’è‰²ç¡®è®¤ï¼‰ï¼Œç›´æ¥ç®€çŸ­å›ç­”",
                "**ä¼˜å…ˆçº§2**: å¦‚æœç¤ºä¾‹å¯¹è¯ä¸­æœ‰é«˜åº¦ç›¸å…³çš„å›ç­”ï¼Œè¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™",
                "**ä¼˜å…ˆçº§3**: å¦‚æœæ˜¯å¼€æ”¾å¼é—®é¢˜ï¼Œå†é€‚åº¦èå…¥å­¦ç”Ÿæ¡£ä½ç‰¹ç‚¹",
                "**æ ¼å¼è¦æ±‚**: ä»…è¿”å›å­¦ç”Ÿå›ç­”å†…å®¹ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚",
                ""
            ])

            user_message = "\n".join(sections)

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            # æ ¹æ®é…ç½®é€‰æ‹©è°ƒç”¨æ–¹å¼
            if self.model_type == "doubao_post":
                print("ğŸ”„ ä½¿ç”¨ Doubao POST API è°ƒç”¨...")
                answer = self._call_doubao_post(messages, temperature=0.7, max_tokens=1000)
            else:  # doubao_sdk
                print("ğŸ”„ ä½¿ç”¨ Doubao OpenAI SDK è°ƒç”¨...")
                response = self.doubao_client.chat.completions.create(
                    model=self.doubao_model,
                    messages=messages,
                    temperature=0.7,
                    top_p=0.9
                )
                answer = response.choices[0].message.content

            return answer
        except Exception as e:
            print(f"âŒ è°ƒç”¨ {self.model_type} æ¨¡å‹å¤±è´¥: {str(e)}")
            return None

    def run_semi_interactive(self, task_id, breakpoint_round: int = 0):
        """
        åŠäº¤äº’å¼è¿è¡Œå·¥ä½œæµï¼š
        - ç”¨æˆ·è¾“å…¥å†…å®¹ä¸ä¸ºç©ºæ—¶ï¼ŒæŒ‰ç”¨æˆ·è¾“å…¥èµ°æµç¨‹
        - ç”¨æˆ·ç›´æ¥å›è½¦ï¼ˆè¾“å…¥ä¸ºç©ºï¼‰æ—¶ï¼Œè®© Doubao æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå›ç­”ï¼ˆé»˜è®¤å¥½å­¦ç”Ÿï¼‰
        - ç”¨æˆ·è¾“å…¥ 'continue' æ—¶ï¼Œåç»­å…¨éƒ¨è‡ªåŠ¨è®©æ¨¡å‹å›ç­”
        - ç”¨æˆ·è¾“å…¥ 'continue N' æ—¶ï¼Œè‡ªåŠ¨è¿è¡Œåˆ°ç¬¬ N è½®åæ¢å¤åŠäº¤äº’

        Args:
            task_id: ä»»åŠ¡ID
            breakpoint_round: æ–­ç‚¹è½®æ•°ï¼Œ0è¡¨ç¤ºä¸è®¾æ–­ç‚¹
        """
        if not self.doubao_client and self.model_type == "doubao_sdk":
            print("\nâŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ ARK_API_KEY ç¯å¢ƒå˜é‡")
            return

        # å¦‚æœæœªè®¾ç½®å­¦ç”Ÿæ¡£ä½ï¼Œé»˜è®¤ä½¿ç”¨"å¥½å­¦ç”Ÿ"
        if not self.student_profile_key:
            print("\nğŸ“š åŠäº¤äº’æ¨¡å¼é»˜è®¤ä½¿ç”¨'ä¼˜ç§€å­¦ç”Ÿ'æ¡£ä½ç”Ÿæˆå›ç­”")
            self.student_profile_key = "good"

        try:
            self.start_workflow(task_id)
            round_num = 1
            auto_continue = False  # æ˜¯å¦è¿›å…¥å…¨è‡ªåŠ¨æ¨¡å¼
            current_breakpoint = breakpoint_round  # å½“å‰æ–­ç‚¹è½®æ•°

            while True:
                if self.current_step_id is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼æ²¡æœ‰æ›´å¤šæ­¥éª¤äº†ã€‚")
                    break

                if round_num > 80:
                    print(f"\nâš ï¸  è­¦å‘Šï¼šå·²è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ•°ï¼ˆ{round_num}è½®ï¼‰ï¼Œè‡ªåŠ¨é€€å‡ºé˜²æ­¢æ— é™å¾ªç¯")
                    break

                print("\n" + "=" * 60)
                mode_label = "å…¨è‡ªåŠ¨æ¨¡å¼" if auto_continue else "åŠäº¤äº’æ¨¡å¼"
                print(f"ğŸ’¬ ç¬¬ {round_num} è½®å¯¹è¯ï¼ˆ{mode_label}ï¼‰")
                print("=" * 60)

                if auto_continue:
                    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æ–­ç‚¹
                    if current_breakpoint > 0 and round_num >= current_breakpoint:
                        print(f"\nğŸ”´ åˆ°è¾¾æ–­ç‚¹ï¼ˆç¬¬ {current_breakpoint} è½®ï¼‰ï¼Œåˆ‡å›åŠäº¤äº’æ¨¡å¼")
                        auto_continue = False
                        current_breakpoint = 0  # æ¸…é™¤æ–­ç‚¹
                        # ä¸ continueï¼Œç»§ç»­èµ°ä¸‹é¢çš„åŠäº¤äº’é€»è¾‘
                    else:
                        # å…¨è‡ªåŠ¨æ¨¡å¼ï¼šç›´æ¥è®©æ¨¡å‹ç”Ÿæˆå›ç­”
                        bp_info = f"ï¼ˆæ–­ç‚¹: ç¬¬ {current_breakpoint} è½®ï¼‰" if current_breakpoint > 0 else ""
                        print(f"\nğŸ¤– æ­£åœ¨ä½¿ç”¨ Doubao ç”Ÿæˆå›ç­”...{bp_info}")
                        answer = self.generate_answer_with_doubao(self.question_text)
                        if not answer:
                            print("âŒ æ— æ³•ç”Ÿæˆå›ç­”ï¼Œé€€å‡ºè‡ªåŠ¨æ¨¡å¼")
                            auto_continue = False
                            continue
                        print(f"ğŸ¤– Doubao ç”Ÿæˆçš„å›ç­”: {answer}")

                if not auto_continue:
                    # åŠäº¤äº’æ¨¡å¼ï¼šç­‰å¾…ç”¨æˆ·è¾“å…¥
                    print("\næç¤ºï¼šå›è½¦=AIå›ç­” | è¾“å…¥å†…å®¹=æ‰‹åŠ¨å›ç­” | continue [N]=å…¨è‡ªåŠ¨(å¯é€‰æ–­ç‚¹) | quit=é€€å‡º")
                    user_input = input("è¯·è¾“å…¥ä½ çš„å›ç­”: ").strip()

                    if user_input.lower() == "quit":
                        print("ğŸ‘‹ ç”¨æˆ·ä¸»åŠ¨é€€å‡º")
                        break

                    if user_input.lower().startswith("continue"):
                        # è§£ææ˜¯å¦å¸¦æ–­ç‚¹å‚æ•°: "continue" æˆ– "continue 10"
                        parts = user_input.split()
                        if len(parts) >= 2:
                            try:
                                current_breakpoint = int(parts[1])
                                if current_breakpoint <= round_num:
                                    print(f"âš ï¸  æ–­ç‚¹å¿…é¡»å¤§äºå½“å‰è½®æ•°ï¼ˆ{round_num}ï¼‰ï¼Œå·²å¿½ç•¥æ–­ç‚¹è®¾ç½®")
                                    current_breakpoint = 0
                                else:
                                    print(f"\nğŸš€ è¿›å…¥å…¨è‡ªåŠ¨æ¨¡å¼ï¼Œå°†åœ¨ç¬¬ {current_breakpoint} è½®åæš‚åœ...")
                            except ValueError:
                                print(f"âš ï¸  æ— æ•ˆçš„æ–­ç‚¹æ•°å­—: {parts[1]}ï¼Œå°†æŒç»­å…¨è‡ªåŠ¨è¿è¡Œ")
                                current_breakpoint = 0
                        else:
                            current_breakpoint = 0
                            print("\nğŸš€ è¿›å…¥å…¨è‡ªåŠ¨æ¨¡å¼ï¼Œåç»­å°†ç”± AI è‡ªåŠ¨å›ç­”...")

                        auto_continue = True
                        # æœ¬è½®ä¹Ÿè‡ªåŠ¨å›ç­”
                        print(f"\nğŸ¤– æ­£åœ¨ä½¿ç”¨ Doubao ç”Ÿæˆå›ç­”...")
                        answer = self.generate_answer_with_doubao(self.question_text)
                        if not answer:
                            print("âŒ æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥")
                            auto_continue = False
                            continue
                        print(f"ğŸ¤– Doubao ç”Ÿæˆçš„å›ç­”: {answer}")
                    elif user_input:
                        # ç”¨æˆ·æœ‰è¾“å…¥ï¼Œä½¿ç”¨ç”¨æˆ·çš„å›ç­”
                        print(f"\nğŸ‘¤ ä½¿ç”¨ç”¨æˆ·å›ç­”: {user_input}")
                        answer = user_input
                    else:
                        # ç”¨æˆ·ç›´æ¥å›è½¦ï¼Œä½¿ç”¨ Doubao ç”Ÿæˆå›ç­”
                        print(f"\nğŸ¤– æ­£åœ¨ä½¿ç”¨ Doubao ç”Ÿæˆå›ç­”...")
                        answer = self.generate_answer_with_doubao(self.question_text)
                        if not answer:
                            print("âŒ æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥")
                            continue
                        print(f"ğŸ¤– Doubao ç”Ÿæˆçš„å›ç­”: {answer}")

                # ä¿å­˜å½“å‰è½®å¯¹è¯åˆ°å†å²
                self.conversation_history.append({
                    "ai": self.question_text,
                    "student": answer
                })

                # å‘é€å›ç­”
                try:
                    result = self.chat(answer)
                except Exception as e:
                    print(f"\nâš ï¸  å‘é€å›ç­”å¤±è´¥: {str(e)}")
                    break

                # æ£€æŸ¥è¿”å›ç»“æœ
                data = (result or {}).get("data") or {}
                if data.get("text") is None and data.get("nextStepId") is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼")
                    break

                round_num += 1
                time.sleep(0.5)

            print("\n" + "=" * 60)
            print("ğŸ‰ å·¥ä½œæµæµ‹è¯•ç»“æŸ")
            print("=" * 60)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            try:
                self._finalize_workflow()
            except Exception:
                pass

    def run_with_doubao(self, task_id):
        """
        ä½¿ç”¨ Doubao æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå›ç­”å¹¶è¿è¡Œå·¥ä½œæµ
        """
        if not self.doubao_client and self.model_type == "doubao_sdk":
            print("\nâŒ Doubao å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ ARK_API_KEY ç¯å¢ƒå˜é‡")
            return

        if not self.student_profile_key:
            print("\nâš ï¸  æœªæŒ‡å®šå­¦ç”Ÿæ¡£ä½ï¼Œé»˜è®¤ä½¿ç”¨'éœ€è¦å¼•å¯¼çš„å­¦ç”Ÿ'ã€‚")
            self.student_profile_key = "medium"

        try:
            # å¯åŠ¨å·¥ä½œæµ
            self.start_workflow(task_id)

            round_num = 1

            # å¾ªç¯å¯¹è¯
            while True:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€æ­¥
                if self.current_step_id is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼æ²¡æœ‰æ›´å¤šæ­¥éª¤äº†ã€‚")
                    break

                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
                if round_num > 80:
                    print(f"\nâš ï¸  è­¦å‘Šï¼šå·²è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ•°ï¼ˆ{round_num}è½®ï¼‰ï¼Œè‡ªåŠ¨é€€å‡ºé˜²æ­¢æ— é™å¾ªç¯")
                    break

                print("\n" + "="*60)
                mode = "æ—¥å¿—å›æ”¾" if self.use_replay_mode else "Doubao è‡ªä¸»å›ç­”"
                print(f"ğŸ¤– ç¬¬ {round_num} è½®å¯¹è¯ï¼ˆ{mode}ï¼‰")
                print("="*60)

                # ä½¿ç”¨å›æ”¾æ¨¡å¼æˆ– Doubao ç”Ÿæˆå›ç­”
                print(f"\nğŸ”„ æ­£åœ¨ç”Ÿæˆå›ç­”...")
                generated_answer = self.generate_answer_with_replay(self.question_text)

                if not generated_answer:
                    print("âŒ æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè·³è¿‡æ­¤è½®")
                    break

                step_id = getattr(self, "current_step_id", None)
                source = "æ—¥å¿—" if self.use_replay_mode and self.replay_engine and self.replay_engine.get_match_info(self.question_text, step_id=step_id).get("matched") else "Doubao"
                print(f"\nğŸ¤– {source} ç”Ÿæˆçš„å›ç­”: {generated_answer}")

                # ä¿å­˜å½“å‰è½®å¯¹è¯åˆ°å†å²
                self.conversation_history.append({
                    "ai": self.question_text,
                    "student": generated_answer
                })

                # å‘é€ç”Ÿæˆçš„å›ç­”
                try:
                    result = self.chat(generated_answer)
                except Exception as e:
                    print(f"\nâš ï¸  å‘é€å›ç­”å¤±è´¥: {str(e)}")
                    break

                # æ£€æŸ¥è¿”å›ç»“æœï¼Œå¦‚æœ text ä¸º null ä¸” nextStepId ä¸º nullï¼Œä»£è¡¨è¾“å‡ºç»“æŸ
                data = (result or {}).get("data") or {}
                if data.get("text") is None and data.get("nextStepId") is None:
                    print("\nâœ… å·¥ä½œæµå®Œæˆï¼")
                    break

                round_num += 1
                time.sleep(1)  # ç¨å¾®å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«

            print("\n" + "="*60)
            print("ğŸ‰ å·¥ä½œæµæµ‹è¯•ç»“æŸ")
            print("="*60)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            # Ensure JSON logs are written even if the last round errors out.
            try:
                self._finalize_workflow()
            except Exception:
                pass


# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("="*60)
    print("ğŸ“‹ å¯¹è¯å·¥ä½œæµè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…· v2.0")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = WorkflowTester()
    
    # æµ‹è¯•è¿æ¥
    if not tester.test_connection():
        print("\nâŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·å…ˆè§£å†³é—®é¢˜")
        exit(1)
    
    # è·å– task_id
    task_id = os.getenv("TASK_ID")
    if not task_id:
        task_id = input("\nè¯·è¾“å…¥ task_id: ").strip()
        if not task_id:
            print("âŒ task_id ä¸èƒ½ä¸ºç©º")
            exit(1)
    
    print(f"\nä½¿ç”¨ task_id: {task_id}")

    # é€‰æ‹©æ—¥å¿—æ ¼å¼
    tester.log_format = tester._get_log_format_preference()

    # é€‰æ‹© LLM æ¨¡å‹
    print("\nè¯·é€‰æ‹© LLM æ¨¡å‹ï¼š")
    print("1. Doubao (OpenAI SDK)")
    print("2. Doubao (POST API / LLM-Service)")

    model_choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2ï¼Œé»˜è®¤ 2): ").strip()
    if model_choice == "1":
        tester.model_type = "doubao_sdk"
    else:
        tester.model_type = "doubao_post"

    # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
    tester._initialize_doubao_client()

    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\nè¯·é€‰æ‹©è¿è¡Œæ–¹å¼ï¼š")
    print("1. åŠäº¤äº’å¼è¿è¡Œï¼ˆæ¨èï¼‰- å›è½¦è‡ªåŠ¨å›ç­”ï¼Œè¾“å…¥å†…å®¹åˆ™æ‰‹åŠ¨å›ç­”")
    print("2. è‡ªåŠ¨åŒ–è¿è¡Œï¼ˆéœ€è¦é¢„è®¾ç­”æ¡ˆï¼‰")
    print("3. å¤§æ¨¡å‹è‡ªä¸»é€‰æ‹©å›ç­”ï¼ˆDoubao è‡ªåŠ¨ç”Ÿæˆç­”æ¡ˆï¼‰")
    print("4. å›æ”¾æ¨¡å¼ï¼ˆæ”¯æŒ TXT æ—¥å¿— difflib / JSON æ—¥å¿— embeddingï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2/3/4): ").strip()

    if choice == "1":
        print("\nğŸ¯ åŠäº¤äº’æ¨¡å¼")
        print("=" * 60)
        print("è¯´æ˜ï¼š")
        print("- ç›´æ¥å›è½¦ï¼šè®© Doubao æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå›ç­”ï¼ˆé»˜è®¤ä¼˜ç§€å­¦ç”Ÿï¼‰")
        print("- è¾“å…¥å†…å®¹ï¼šä½¿ç”¨ä½ è¾“å…¥çš„å†…å®¹ä½œä¸ºå›ç­”")
        print("- è¾“å…¥ continueï¼šåç»­å…¨éƒ¨ç”± AI è‡ªåŠ¨å›ç­”")
        print("- è¾“å…¥ continue Nï¼šè‡ªåŠ¨è¿è¡Œåˆ°ç¬¬ N è½®åæš‚åœï¼Œæ¢å¤åŠäº¤äº’")
        print("- è¾“å…¥ quitï¼šé€€å‡ºç¨‹åº")
        print("=" * 60)

        # å¯é€‰ï¼šè®©ç”¨æˆ·é€‰æ‹©å­¦ç”Ÿæ¡£ä½
        print("\nè¯·é€‰æ‹©å­¦ç”Ÿæ¡£ä½ï¼Ÿï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤2.'éœ€è¦å¼•å¯¼çš„å­¦ç”Ÿ'ï¼‰")
        tester.prompt_student_profile()

        # å¯é€‰ï¼šè®¾ç½®åˆå§‹æ–­ç‚¹
        print("\næ˜¯å¦é¢„è®¾æ–­ç‚¹ï¼Ÿï¼ˆåœ¨ç¬¬ N è½®è‡ªåŠ¨æš‚åœï¼Œç›´æ¥å›è½¦ä¸è®¾æ–­ç‚¹ï¼‰")
        bp_input = input("æ–­ç‚¹è½®æ•° (ç›´æ¥å›è½¦è·³è¿‡): ").strip()
        breakpoint_round = 0
        if bp_input:
            try:
                breakpoint_round = int(bp_input)
                if breakpoint_round > 0:
                    print(f"âœ… å·²è®¾ç½®æ–­ç‚¹ï¼šç¬¬ {breakpoint_round} è½®åæš‚åœ")
                else:
                    breakpoint_round = 0
            except ValueError:
                print("âš ï¸  æ— æ•ˆæ•°å­—ï¼Œä¸è®¾ç½®æ–­ç‚¹")

        # å¯é€‰ï¼šåŠ è½½çŸ¥è¯†åº“
        print("\nå¯é€‰: æ˜¯å¦ä½¿ç”¨å¤–æ¥çŸ¥è¯†åº“æˆ–è€…å¯¹è¯ç¤ºä¾‹æ–‡æ¡£ï¼Ÿ")
        use_kb = input("æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“æˆ–è€…å¯¹è¯ç¤ºä¾‹æ–‡æ¡£ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_kb == "y":
            kb_path = input("\nè¯·è¾“å…¥å¯¹åº”çš„ Markdownæˆ–è€…docx æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if kb_path:
                if not tester.load_knowledge_base(kb_path):
                    print("âš ï¸  çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼Œå°†ä»¥é€šç”¨æ¨¡å¼è¿è¡Œ")
            else:
                print("âš ï¸  æœªæä¾›çŸ¥è¯†åº“è·¯å¾„ï¼Œè·³è¿‡åŠ è½½")

        tester.run_semi_interactive(task_id, breakpoint_round=breakpoint_round)

    elif choice == "2":
        print("\næç¤º: è¯·å…ˆåœ¨ä»£ç ä¸­é…ç½® user_answers åˆ—è¡¨")
        user_answers = [
            "è¿™æ˜¯ç¬¬ä¸€ä¸ªç­”æ¡ˆ",
            "è¿™æ˜¯ç¬¬äºŒä¸ªç­”æ¡ˆ",
            "è¿™æ˜¯ç¬¬ä¸‰ä¸ªç­”æ¡ˆ"
        ]
        tester.run_auto(task_id, user_answers)

    elif choice == "3":
        print("\nğŸ¤– ä½¿ç”¨ Doubao æ¨¡å‹è‡ªä¸»å›ç­”æ¨¡å¼")
        tester.prompt_student_profile()

        print("\nå¯é€‰: æ˜¯å¦æä¾›å­¦ç”Ÿæ¡£ä½æ¨¡æ‹Ÿå¯¹è¯ Markdownï¼Ÿ")
        use_dialogue_md = input("æ˜¯å¦åŠ è½½æ¨¡æ‹Ÿå¯¹è¯ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_dialogue_md == "y":
            dialogue_path = input("\nè¯·è¾“å…¥ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if dialogue_path:
                tester.load_student_dialogues(dialogue_path)
            else:
                print("âš ï¸  æœªæä¾›è·¯å¾„ï¼Œè·³è¿‡åŠ è½½æ¨¡æ‹Ÿå¯¹è¯")

        # å¯é€‰ï¼šåŠ è½½çŸ¥è¯†åº“
        print("\nå¯é€‰: æ˜¯å¦ä½¿ç”¨å¤–æ¥çŸ¥è¯†åº“æˆ–è€…å¯¹è¯ç¤ºä¾‹æ–‡æ¡£ï¼Ÿ")
        use_kb = input("æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“æˆ–è€…å¯¹è¯ç¤ºä¾‹æ–‡æ¡£ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_kb == "y":
            kb_path = input("\nè¯·è¾“å…¥å¯¹åº”çš„ Markdownæˆ–è€…docx æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if kb_path:
                if not tester.load_knowledge_base(kb_path):
                    print("âš ï¸  çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼Œå°†ä»¥é€šç”¨æ¨¡å¼è¿è¡Œ")
            else:
                print("âš ï¸  æœªæä¾›çŸ¥è¯†åº“è·¯å¾„ï¼Œè·³è¿‡åŠ è½½")

        print("\nå¼€å§‹å·¥ä½œæµ...")
        tester.run_with_doubao(task_id)

    elif choice == "4":
        print("\nğŸ¯ æ—¥å¿—å›æ”¾æ¨¡å¼")
        print("="*60)
        print("è¯´æ˜ï¼š")
        print("1. ç¬¬ä¸€æ¬¡è¿è¡Œç”Ÿæˆå¯¹è¯æ—¥å¿—æˆ–å¯¼å‡ºå¯¹è¯ JSON")
        print("2. æ‰‹åŠ¨ä¿®æ”¹å…¶ä¸­çš„ç”¨æˆ·å›ç­”ï¼ˆå¦‚éœ€è¦ï¼‰")
        print("3. å†æ¬¡è¿è¡Œæ—¶ï¼Œç¨‹åºä¼šæ ¹æ®AIæé—®æ‰¾åˆ°æœ€åŒ¹é…çš„å†å²æé—®")
        print("   å¹¶å¼ºåˆ¶ä½¿ç”¨å¯¹åº”çš„ç”¨æˆ·å›ç­”")
        print("4. æ‰¾ä¸åˆ°åŒ¹é…æ—¶ï¼Œæ‰è®©æ¨¡å‹è‡ªå·±ç”Ÿæˆå›ç­”")
        print("="*60)

        # è¾“å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„
        log_path = input("\nè¯·è¾“å…¥å¯¹è¯æ—¥å¿—æ–‡ä»¶è·¯å¾„ (*_dialogue.txt æˆ– *_dialogue.json): ").strip()
        if not log_path:
            print("âŒ æ—¥å¿—æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º")
            exit(1)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(log_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
            exit(1)

        # é…ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
        default_threshold = 0.8 if log_path.lower().endswith(".json") else 0.7
        threshold_input = input(f"\nè¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0ï¼Œé»˜è®¤ {default_threshold}): ").strip()
        similarity_threshold = default_threshold
        if threshold_input:
            try:
                similarity_threshold = float(threshold_input)
                if similarity_threshold < 0.0 or similarity_threshold > 1.0:
                    print(f"âš ï¸  é˜ˆå€¼å¿…é¡»åœ¨0.0-1.0ä¹‹é—´ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_threshold}")
                    similarity_threshold = default_threshold
            except ValueError:
                print(f"âš ï¸  æ— æ•ˆçš„é˜ˆå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼{default_threshold}")

        # é€‰æ‹©å­¦ç”Ÿæ¡£ä½
        tester.prompt_student_profile()

        # å¯ç”¨å›æ”¾æ¨¡å¼
        tester.enable_replay_mode(log_path, similarity_threshold)

        print("\nå¯é€‰: æ˜¯å¦æä¾›å­¦ç”Ÿæ¡£ä½æ¨¡æ‹Ÿå¯¹è¯ Markdownï¼Ÿ")
        use_dialogue_md = input("æ˜¯å¦åŠ è½½æ¨¡æ‹Ÿå¯¹è¯ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_dialogue_md == "y":
            dialogue_path = input("\nè¯·è¾“å…¥ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if dialogue_path:
                tester.load_student_dialogues(dialogue_path)
            else:
                print("âš ï¸  æœªæä¾›è·¯å¾„ï¼Œè·³è¿‡åŠ è½½æ¨¡æ‹Ÿå¯¹è¯")

        print("\nå¯é€‰: æ˜¯å¦ä½¿ç”¨å¤–æ¥çŸ¥è¯†åº“ï¼Ÿ")
        use_kb = input("æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“ï¼Ÿ(y/nï¼Œé»˜è®¤ n): ").strip().lower()
        if use_kb == "y":
            kb_path = input("\nè¯·è¾“å…¥çŸ¥è¯†åº“ Markdown æ–‡ä»¶çš„ç»å¯¹è·¯å¾„: ").strip()
            if kb_path:
                if not tester.load_knowledge_base(kb_path):
                    print("âš ï¸  çŸ¥è¯†åº“åŠ è½½å¤±è´¥ï¼Œå°†ä»¥é€šç”¨æ¨¡å¼è¿è¡Œ")
            else:
                print("âš ï¸  æœªæä¾›çŸ¥è¯†åº“è·¯å¾„ï¼Œè·³è¿‡åŠ è½½")

        print("\nå¼€å§‹å·¥ä½œæµ...")
        tester.run_with_doubao(task_id)

    else:
        print("âŒ æ— æ•ˆé€‰é¡¹")
