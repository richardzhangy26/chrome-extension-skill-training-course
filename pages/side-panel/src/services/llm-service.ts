/**
 * LLM æœåŠ¡ - è°ƒç”¨è±†åŒ…æ¨¡å‹ç”Ÿæˆå›ç­”
 * å‚è€ƒ Python: auto_script_train.py ä¸­çš„ _call_doubao_post æ–¹æ³•
 */

import { DEFAULT_PROFILE_KEY, DEFAULT_SYSTEM_PROMPT, STUDENT_PROFILES, llmConfigStorage } from '@extension/storage';
import type { LLMConfig } from '@extension/storage';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface LLMResponse {
  success: boolean;
  content?: string;
  error?: string;
}

const resolveSystemPrompt = (config: LLMConfig) => {
  if (config.systemPromptMode === 'custom' && config.systemPrompt.trim()) {
    return config.systemPrompt.trim();
  }

  return DEFAULT_SYSTEM_PROMPT;
};

const resolveStudentProfile = (config: LLMConfig) =>
  STUDENT_PROFILES[config.studentProfileKey] ?? STUDENT_PROFILES[DEFAULT_PROFILE_KEY];

const buildUserMessage = (aiQuestion: string, profile: { label: string; description: string; style: string }) => {
  const sections = [
    '## è§’è‰²è®¾å®š',
    `å­¦ç”Ÿæ¡£ä½: ${profile.label}`,
    `è§’è‰²ç‰¹å¾: ${profile.description}`,
    `è¡¨è¾¾é£æ ¼: ${profile.style}`,
    '',
    '## é—®é¢˜ç±»å‹è¯†åˆ«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰',
    'å¦‚æœå½“å‰é—®é¢˜å±äºä»¥ä¸‹ç±»å‹ï¼Œè¯·ä¼˜å…ˆç›´æ¥å›ç­”ï¼Œä¸éœ€è¦å¼ºåˆ¶ä½“ç°æ€§æ ¼ç‰¹ç‚¹ï¼š',
    "1. **ç¡®è®¤å¼é—®é¢˜**: å¦‚'ä½ å‡†å¤‡å¥½äº†å—ï¼Ÿè¯·å›å¤æ˜¯æˆ–å¦'ã€'ç¡®è®¤çš„è¯è¯·å›å¤æ˜¯'",
    "   â†’ ç›´æ¥å›ç­”'æ˜¯'ã€'å¥½çš„'ã€'ç¡®è®¤'ç­‰",
    "2. **é€‰æ‹©å¼é—®é¢˜**: å¦‚'ä½ é€‰æ‹©Aè¿˜æ˜¯Bï¼Ÿ'ã€'è¯·é€‰æ‹©1/2/3'",
    "   â†’ ç›´æ¥è¯´å‡ºé€‰é¡¹ï¼Œå¦‚'æˆ‘é€‰æ‹©A'ã€'é€‰1'",
    "3. **è§’è‰²ç¡®è®¤é—®é¢˜**: å¦‚'ä½ æ˜¯å­¦ç”Ÿè¿˜æ˜¯è€å¸ˆï¼Ÿ'",
    "   â†’ ç›´æ¥å›ç­”è§’è‰²ï¼Œå¦‚'å­¦ç”Ÿ'",
    '',
    "**åˆ¤æ–­æ ‡å‡†**: å¦‚æœé—®é¢˜ä¸­åŒ…å«'è¯·å›å¤'ã€'è¯·é€‰æ‹©'ã€'æ˜¯æˆ–å¦'ã€'A/B/C'ç­‰æ˜ç¡®æŒ‡ç¤ºï¼Œåˆ™ä¸ºå°é—­å¼é—®é¢˜ã€‚",
    '',
  ];

  sections.push(
    '## å½“å‰é—®é¢˜',
    aiQuestion,
    '',
    '## è¾“å‡ºè¦æ±‚ï¼ˆæŒ‰ä¼˜å…ˆçº§æ‰§è¡Œï¼‰',
    '**ä¼˜å…ˆçº§1**: å¦‚æœæ˜¯å°é—­å¼é—®é¢˜ï¼ˆç¡®è®¤å¼/é€‰æ‹©å¼/è§’è‰²ç¡®è®¤ï¼‰ï¼Œç›´æ¥ç®€çŸ­å›ç­”',
    '**ä¼˜å…ˆçº§2**: å¦‚æœç¤ºä¾‹å¯¹è¯ä¸­æœ‰é«˜åº¦ç›¸å…³çš„å›ç­”ï¼Œè¯·ä¼˜å…ˆå¼•ç”¨æˆ–æ”¹å†™',
    '**ä¼˜å…ˆçº§3**: å¦‚æœæ˜¯å¼€æ”¾å¼é—®é¢˜ï¼Œå†é€‚åº¦èå…¥å­¦ç”Ÿæ¡£ä½ç‰¹ç‚¹',
    '**æ ¼å¼è¦æ±‚**: ä»…è¿”å›å­¦ç”Ÿå›ç­”å†…å®¹ï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œæ§åˆ¶åœ¨50å­—ä»¥å†…ã€‚',
    '',
  );

  return sections.join('\n');
};

/**
 * è°ƒç”¨è±†åŒ…æ¨¡å‹ç”Ÿæˆå­¦ç”Ÿå›ç­”
 */
const generateStudentAnswer = async (
  aiQuestion: string,
  conversationHistory: Array<{ ai: string; student: string }> = [],
): Promise<LLMResponse> => {
  // è·å–é…ç½®
  const config = await llmConfigStorage.get();

  if (!config.apiKey) {
    return { success: false, error: 'è¯·å…ˆé…ç½® LLM API Key' };
  }

  try {
    const systemPrompt = resolveSystemPrompt(config);
    const profile = resolveStudentProfile(config);
    const userMessage = buildUserMessage(aiQuestion, profile);

    const historyMessages: ChatMessage[] = [];
    for (const turn of conversationHistory.slice(-5)) {
      historyMessages.push({ role: 'assistant', content: turn.ai });
      historyMessages.push({ role: 'user', content: turn.student });
    }

    const messages: ChatMessage[] = [
      { role: 'system', content: systemPrompt },
      ...historyMessages,
      { role: 'user', content: userMessage },
    ];

    // è°ƒç”¨ API
    const response = await fetch(config.apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'api-key': config.apiKey,
        'service-code': config.serviceCode,
      },
      body: JSON.stringify({
        model: config.model,
        messages,
        temperature: 0.7,
        max_tokens: 200,
        top_p: 0.9,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('LLM API Error:', response.status, errorText);
      return { success: false, error: `API è¯·æ±‚å¤±è´¥: ${response.status}` };
    }

    const data = await response.json();
    const content = data?.choices?.[0]?.message?.content?.trim();

    if (!content) {
      return { success: false, error: 'æ¨¡å‹æœªè¿”å›æœ‰æ•ˆå†…å®¹' };
    }

    console.log('ğŸ¤– LLM ç”Ÿæˆå›ç­”:', content);
    return { success: true, content };
  } catch (error) {
    console.error('LLM Service Error:', error);
    return { success: false, error: `è°ƒç”¨å¤±è´¥: ${(error as Error).message}` };
  }
};

/**
 * æµ‹è¯• LLM é…ç½®æ˜¯å¦æœ‰æ•ˆ
 */
const testLLMConfig = async (config: LLMConfig): Promise<LLMResponse> => {
  try {
    const response = await fetch(config.apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'api-key': config.apiKey,
        'service-code': config.serviceCode,
      },
      body: JSON.stringify({
        model: config.model,
        messages: [{ role: 'user', content: 'ä½ å¥½' }],
        max_tokens: 10,
      }),
    });

    if (!response.ok) {
      return { success: false, error: `API è¿æ¥å¤±è´¥: ${response.status}` };
    }

    return { success: true, content: 'é…ç½®æœ‰æ•ˆ' };
  } catch (error) {
    return { success: false, error: `è¿æ¥å¤±è´¥: ${(error as Error).message}` };
  }
};

export { generateStudentAnswer, testLLMConfig };
